# Self-Rewarding Language Models

 Weizhe Yuan\({}^{1,2}\) Richard Yuanzhe Pang\({}^{1,2}\)경현 Cho\({}^{2}\)

**Sainbayar Sukhbaatar\({}^{1}\) Jing Xu\({}^{1}\) Jason Weston\({}^{1,2}\)**

\({}^{1}\) Meta \({}^{2}\) NYU

###### Abstract

우리는 초인간 에이전트를 달성하기 위해 미래 모델은 적절한 훈련 신호를 제공하기 위해 초인간 피드백을 필요로 한다고 가정한다. 현재 접근법은 일반적으로 인간 선호도로부터 보상 모델을 훈련하며, 이는 인간 성능 수준에 의해 병목 현상이 발생할 수 있으며, 두 번째로 이러한 별도의 동결된 보상 모델은 LLM 훈련 동안 개선하는 방법을 배울 수 없다. 이 연구에서는 언어 모델 자체가 LLM-as-a-판단을 통해 사용되어 훈련 중에 자체 보상을 제공하는 _자체 보상 언어 모델_을 연구한다. 우리는 반복적 DPO 교육 동안 수업 추종 능력이 향상될 뿐만 아니라 자신에게 고품질 보상을 제공하는 능력도 향상됨을 보여준다. Llama 2 70B를 세 번의 반복으로 미세 조정하면 Claude 2, Gemini Pro 및 GPT-4 0613을 포함하여 알파카에발 2.0 리더보드의 많은 기존 시스템보다 성능이 우수한 모델이 생성된다. 예비 연구이지만 이 작업은 두 축 모두에서 지속적으로 개선할 수 있는 모델의 가능성에 대한 문을 연다.

## 1 Introduction

인간 선호 데이터를 사용하여 LMM(Large Language Models)을 정렬하는 것은 사전 훈련된 모델의 성능에 따르는 명령어를 크게 개선할 수 있다(Ouyang et al., 2022; Bai et al., 2022). 인간 피드백으로부터 강화 학습(RLHF: Reinforcement Learning from Human Feedback)의 표준 접근법은 이러한 인간의 선호로부터 보상 모델을 학습한다. 그런 다음, 보상 모델은 동결되고, 예를 들어 PPO를 통해 RL을 사용하여 LLM을 트레이닝하는 데 사용된다(Schulman 등, 2017). 최근의 대안은, 직접 선호도 최적화(DPO; Rafailov et al., 2023)에서와 같이, 보상 모델을 전혀 트레이닝하는 것을 피하고, 인간 선호도를 직접 사용하여 LLM을 트레이닝하는 것이다. 두 경우 모두 인간 선호 데이터의 크기와 품질에 의해 접근 방식이 병목되며, RLHF의 경우 그들로부터 훈련된 동결 보상 모델의 품질도 병목한다.

이 작업에서 우리는 대신 이러한 병목 현상을 피하기 위해 동결되기보다는 LLM 정렬 동안 지속적으로 업데이트되는 자체 개선 보상 모델을 훈련할 것을 제안한다. 이러한 접근법의 핵심은 보상 모델과 언어 모델과 같은 별개의 모델로 분리하는 것이 아니라 훈련 중에 원하는 모든 능력을 보유한 에이전트를 개발하는 것이다. 명령어 추종 태스크의 프리트레이닝 및 멀티태스킹 트레이닝이 한번에 많은 태스크에 대한 트레이닝에 의한 태스크 전달을 허용하는 것과 동일한 방식으로(Collobert and Weston, 2008; Radford et al., 2019; Ouyang et al., 2022), 보상 모델을 동일한 시스템에 통합하면 보상 모델링 태스크와 명령어 추종 태스크 사이의 태스크 전달을 허용한다.

따라서, (i) 둘 다 주어진 프롬프트에 대한 응답을 생성하는 명령어 추종 모델로서 작용하는 에이전트들 _자기 보상 언어 모델_ 을 도입하고, (ii) 그들 자신의 트레이닝 세트에 추가하기 위해 새로운 명령어 추종 예들을 생성하고 평가할 수 있다. 우리는 Xu 등(2023)에 최근에 소개된 것과 유사한 Iterative DPO 프레임워크를 사용하여 이러한 모델들을 훈련시킨다. 시드 모델로부터 시작하여, 도 1에 도시된 바와 같이, 각각의 반복에서, 후보 응답들이 새로 생성된 프롬프트들에 대한 모델에 의해 생성되고, 그 후 동일한 모델에 의해 보상이 할당되는 _셀프-명령 생성_의 프로세스가 존재한다. 후자는 LLM-as-a-Judge 프롬프팅을 통해 구현되며, 이는 작업에 따른 지시라고도 볼 수 있다. 생성된 데이터로부터 선호도 데이터셋이 구축되고, 모델의 다음 반복은 DPO를 통해 트레이닝된다.

우리의 실험에서, Llama 2 70B (Touvron et al., 2023) 시드 모델이 Open Assistant (Kopf et al., 2023) 상에서 미세 조정된 것으로 시작하여, 상기 트레이닝 스킴을 수행한다. 그 결과, 베이스라인 시드 모델에 비해 Self-Rewarding LLM 정렬에 의한 성능 향상뿐만 아니라 더 이상 고정되지 않은 보상 모델링 능력도 향상됨을 알 수 있었다. 이는 반복 트레이닝 동안의 모델이 주어진 반복에서 이전 반복에서보다 자신에게 더 높은 품질의 선호도 데이터세트를 제공할 수 있다는 것을 의미한다. 이 효과는 실제 환경에서 포화될 가능성이 있지만 원래 인간 작성 종자 데이터 단독으로 훈련될 수 있는 것보다 우수한 보상 모델(따라서 LLM)을 얻을 수 있는 흥미로운 가능성을 제공한다.

## 2 Self-Rewarding Language Models

우리의 접근법은 먼저 기본 사전 훈련된 언어 모델과 소량의 인간 주석이 달린 시드 데이터에 대한 액세스를 가정한다. 그런 다음 두 가지 기술을 동시에 소유하는 것을 목표로 하는 모델을 구축합니다.

1. _다음 지침_: 사용자 요청을 설명하는 프롬프트가 주어지면 고품질, 도움이 되는(무해한) 응답을 생성할 수 있습니다.
2. _자체 명령 만들기_: 자체 훈련 세트에 추가 하기 위해 새 명령 후속 예제를 생성 하 고 평가 하는 기능입니다.

이러한 기술은 모델이 자기 정렬을 수행할 수 있도록 사용되며, 즉 AI 피드백(AIF)을 사용하여 자신을 반복적으로 훈련하는 데 사용되는 구성요소이다.

_자기-지시 생성_은 후보 응답들을 생성한 다음 모델 자체가 그들의 품질을 판단하는 것, 즉 외부 응답에 대한 필요성을 대체하는 자신의 보상 모델로서 동작한다. 이것은 _LLM-as-a-Judge_ 메카니즘(Zheng et al., 2023b)을 통해, 즉, 응답들의 평가를 태스크에 후속하는 명령으로서 공식화함으로써 구현된다. 이 자체 생성된 AIF 선호도 데이터는 트레이닝 세트로 사용된다.

우리의 전반적인 _자기 정렬_ 절차는 반복적이며, 이는 각각이 마지막보다 개선된다는 목표로 일련의 그러한 모델을 구축함으로써 진행된다. 중요한 것은, 모델이 그 생성 능력을 둘 다 향상시킬 수 있고, 동일한 생성 메커니즘을 통해 그 자신의 보상 모델로서 작용하기 때문에, 이것은 보상 모델 자체가 보상 모델이 고정된 표준 관행으로부터 벗어나, 이러한 반복들을 통해 개선될 수 있다는 것을 의미한다(오양).

그림 1: **자체 보상 언어 모델** 입니다. 자기 정렬 방법은 (i) _자기 지시 생성_ 두 단계로 구성된다. 새로 생성된 프롬프트는 모델 \(M_{t}\)으로부터 후보 응답을 생성하는 데 사용되며, 이 프롬프트는 LLM-as-a-Judge 프롬프팅을 통해 자신의 보상을 예측한다. (ii) 훈련에 따른 명령어: DPO를 통한 훈련에 사용되는 생성된 데이터에서 선호도 쌍을 선택하여 모델 \(M_{t+1}\)을 생성한다. 그런 다음이 전체 절차를 반복 하 여 향상된 명령 팔로우 및 보상 모델링 능력을 생성할 수 있습니다. **

et al., 2022). 우리는 이것이 제약적인 병목 현상을 제거하여 앞으로 이러한 학습 모델의 자체 개선 잠재력의 한계를 증가시킬 수 있다고 믿는다.

이하에서 이러한 단계들을 보다 상세하게 설명한다. 접근법의 개요는 그림 1에 나와 있다.

### Initialization

데이터에 따른 시드 지시: 사전 훈련된 기본 언어 모델로부터 시작하여, 감독 미세 조정(supervised fine-tuning; SFT) 방식으로 훈련에 사용하는 예들에 따른 인간-저자(지시 프롬프트, 응답) 일반 지시들의 시드 세트가 주어진다. 이어서, 이를 IFT(Instruction Fine-Tuning) 데이터라 칭하기로 한다.

데이터에 따른 종자 LLM-as-a-판정 지시: 또한 훈련에 사용될 수 있는 (평가 지시 프롬프트, 평가 결과 응답) 예들의 시드 세트가 제공된다고 가정한다. 이것은 엄격하게 필요하지 않지만 IFT 데이터를 사용하는 모델은 이미 LLM-as-a-판사를 훈련시킬 수 있을 것이기 때문에 이러한 훈련 데이터가 개선된 결과를 제공할 수 있음을 보여준다. 이 데이터에서, 입력 프롬프트는 특정 명령에 대한 주어진 응답의 품질을 평가하도록 모델에 요청한다. 제공된 평가 결과 응답은 연쇄 사고 추론(정당화)과 최종 점수(실험 5점 만점)로 구성된다. 이러한 프롬프트에 대해 선택한 형식은 그림 2에 나와 있으며, 이는 LLM이 보상 모델의 역할을 수행하기 위한 훈련 데이터 역할을 한다. 이어서, 이를 EFT(Evaluation Fine-Tuning) 데이터라 칭하기로 한다.

훈련 중에 이 씨앗 세트를 함께 사용합니다.

### Self-Instruction Creation

우리가 훈련한 모델을 사용하여 자체 훈련 세트를 자체 수정할 수 있습니다. 구체적으로, 우리는 다음 반복 학습을 위한 추가 학습 데이터를 생성한다.

이는 다음의 단계들로 구성된다:

1. _새 프롬프트 생성_: Wang 등(2022) 및 Honovich 등(2023)의 접근 방식에 따라 원본 시드 IFT 데이터에서 몇 번 샷 프롬프트, 샘플링 프롬프트를 사용하여 새 프롬프트 \(x_{i}\)를 생성합니다.
2. _후보 응답 생성_: 샘플링을 사용하여 모델로부터 주어진 프롬프트 \(x_{i}\)에 대한 \(N\)다양한 후보 응답 \(\{y_{i}^{1},\dots,y_{i}^{N}\}\)을 생성한다.
3. _후보 응답 평가_: 마지막으로, 동일한 모델의 LLM-as-a-판정 능력을 사용하여 점수 \(r_{i}^{n}\in[0,5]\)으로 자신의 후보 응답을 평가한다(도 2 참조).

### 명령 추적 훈련

앞서 설명한 바와 같이, 트레이닝은 처음에 시드 IFT 및 EFT 데이터로 수행된다(섹션 2.1). 그런 다음 AI(Self-Feedback)를 통해 추가 데이터로 증강된다.

AI 피드백 훈련은 자체 학습 생성 절차를 수행한 후, AI 피드백 훈련(AIFT) 데이터라고 하는 훈련에 대한 추가 예제와 함께 시드 데이터를 보강할 수 있다. 이러한 피드백의 두 가지 변형을 시도합니다.

* _선호 쌍_: 형식의 학습 데이터(명령 프롬프트 \(x_{i}\), 응답 승리 \(y_{i}^{w}\), 응답 손실 \(y_{i}^{l}\))를 구성합니다. 승패 쌍을 형성하기 위해, Xu 등(2023)에 이어, \(N\) 평가된 후보 응답으로부터 최고 및 최저 스코어링 응답을 취하고, 그들의 스코어가 동일한 경우 쌍을 폐기한다(섹션 2.2 참조). 이들 쌍은 선호도 튜닝 알고리즘으로 훈련하는데 사용될 수 있다. 우리는 DPO(Rafailov et al., 2023)를 사용한다.
* _긍정적인 예들만_: 이 변형예에서, 다른 접근법들(Li et al., 2023; Adolphs et al., 2023; Gulcehre et al., 아래에서 설명되는 첨가제 5-포인트 스코어링 시스템을 사용하여 사용자의 질문 및 대응하는 응답을 검토함)에 따라, 감독된 미세조정을 위한 시드 세트에 모델에 의해 큐레이션된 (지시 프롬프트, 응답)의 추가적인 예들을 추가한다. 각 기준의 만족도를 기준으로 포인트가 적립된다:

- 응답이 적절 하 고 불완전 하거나 관련 없는 일부 콘텐츠를 포함 하는 경우에도 사용자의 문의와 관련 된 일부 정보를 제공 하는 경우 1 점을 추가 합니다.

- 응답이 사용자의 질문의 상당 부분을 처리하지만 쿼리를 완전히 해결하거나 직접 답변을 제공하지 않는 경우 다른 점을 추가합니다.

- AI 어시스턴트에 의해 작성된 것으로 보이는지 또는 블로그 또는 검색 결과에서 전형적으로 발견되는 요소를 갖는지에 관계없이 응답이 사용자의 질문의 기본 요소에 유용한 방식으로 응답하는 경우 세 번째 포인트를 수여한다.

- AI 어시스턴트의 관점에서 응답이 명확 하 게 작성 되 면 네 번째 포인트를 부여 하 여 사용자의 질문을 직접 및 포괄적으로 처리 하 고 명확성, 간결성 또는 초점을 개선할 여지가 약간 있더라도 잘 구성 되 고 도움이 됩니다.

- 불필요한 정보 없이 AI 어시스턴트에 의해 사용자의 질문에 완벽하게 맞춰지고, 전문 지식을 반영하며, 고품질, 매력적이고 통찰력 있는 답변을 보여주는 응답에 대한 5번째 포인트를 부여합니다.

User: <INSTRUCTION_HER>

<response><RESPONSE_HER></response>

사용자의 지시 및 응답을 검사한 후:

- 총 점수(최대 100 단어)를 간략하게 정당화합니다.

- "점수: <총점>"의 형식을 이용하여 점수로 마무리한다.

필요에 따라 웹 검색 지식을 활용하여 AI 어시스턴트 관점에서 평가해야 합니다. 이 추가 점수 모델과 일치 하 여 응답을 평가 하려면 개요 된 기준에 따라 체계적으로 속성 점을 평가 합니다.

2023]인 것을 특징으로 하는 선호도 데이터 구축 방법. 이 설정에서는 후보 응답이 \(r_{i}^{n}=5\)의 만점을 제공하도록 평가된 예만 추가한다.

실험에서 두 가지 접근법의 결과를 보고하지만 선호도 쌍에서 학습하는 것이 우수한 성능을 제공하므로 해당 접근법을 권장한다.

### 전체 자기 정렬 알고리즘

반복 훈련의 전반적인 절차는 일련의 모델 \(M_{1},\ldots,M_{T}\)을 훈련하는데, 각각의 연속된 모델 \(t\)은 \(t-1^{\text{th}}\) 모델에 의해 생성된 증강 훈련 데이터를 사용한다. 따라서 AIFT(\(M_{t}\))는 모델 \(M_{t}\)을 사용하여 생성된 AI 피드백 훈련 데이터를 의미하는 것으로 정의한다.

따라서 모델 SequenceWe는 다음과 같이 모델을 정의 하 고 사용 하는 학습 데이터를 다음과 같이 정의 합니다.

\(M_{0}\): 미세 조정 없이 베이스 프리트레이닝된 LLM.

\(M_{1}\): \(M_{0}\)으로 초기화한 후 SFT를 사용하여 IFT+EFT 시드 데이터를 미세 조정한다.

\(M_{2}\): \(M_{1}\)으로 초기화한 후 DPO를 이용하여 AIFT(\(M_{1}\)) 데이터로 학습한다.

\(M_{3}\): \(M_{2}\)으로 초기화한 후 DPO를 이용하여 AIFT(\(M_{2}\)) 데이터로 학습한다.

이 반복 훈련은 Xu 등(2023)에 소개된 Pairwise Cringe Optimization 및 Iterative DPO에서 사용된 절차와 유사하지만 외부 고정 보상 모델이 해당 작업에 사용되었다.

그림 2: **LLM-as-a-판정 프롬프트는 LLM이 보상 모델 역할을 하고 자체 모델 세대에 대해 자체 보상을 제공 합니다. 이 모델은 처음에 이 작업에서 잘 수행하는 방법에 대한 시드 훈련 데이터로 훈련된 다음 우리의 자체 보상 훈련 절차를 통해 이 작업에서 더 개선된다.

Experiments

### Experimental Setup

기본 모델에서는 Llama 2 70B (Touvron et al., 2023)를 기본 사전 훈련 모델로 사용한다.

#### 3.1.1 Seed Training Data

IFT Seed DataWe used the human-authored examples provided in Open Assistant dataset (Kopf et al., 2023) for instruction fine-tuning. Li 등(2023)에 이어, 우리는 3200개의 예를 사용하여, 그들의 인간 주석된 랭크(가장 높은 랭크 0만을 선택함)에 기초하여 고품질인 영어의 첫 번째 회화 턴만을 샘플링함으로써. 실험에서는 감독 미세 조정을 통해 이 데이터만 사용하여 기본 모델에서 미세 조정된 모델과 비교하고 _SFT 기준선_이라고 한다.

EFT 시드 데이터 오픈 어시스턴트 데이터는 또한 평가 미세 조정 데이터를 구성할 수 있는 프롬프트당 여러 순위가 매겨진 인간 응답을 제공한다. 우리는 이것을 열차와 평가 세트로 나누어 LLM-as-a-판정 데이터를 만드는 데 사용한다. 이는 채점 기준 설명으로 구성된 그림 2에 주어진 입력 형식으로 배치하여 수행되며 평가해야 할 주어진 명령과 응답.1 훈련 대상의 경우 5개 중 체인의 사유 정당화와 최종 점수가 직접 제공되지 않으므로 SFT 기준선을 사용하여 각 입력에 대해 이러한 출력 평가를 생성하고 점수의 순위가 데이터 세트의 인간 순위와 일치하면 훈련 세트에 수용한다. 이는 1775개의 트레인 및 531개의 평가 예들(IFT 데이터와 겹치지 않음)을 초래한다.

각주 1: 참고, Li 등(2023)으로부터 도출된 프롬프트는 "웹 검색을 이용하는 것"을 언급하지만, 우리의 모델은 실제로 이러한 동작을 할 수 없다.

#### 3.1.2 Evaluation Metrics

우리는 자기 보상 모델의 성능을 지침을 따르는 능력과 보상 모델로서의 능력(응답을 평가하는 능력)의 두 축으로 평가한다.

지침 Following우리는 GPT-4(Achiam et al., 2023)를 평가자로 사용하여 AlpacaEval 평가 프롬프트(Li et al., 2023)를 사용하여 Li et al.(2023)에 이어 다양한 소스로부터 유도된 256개의 테스트 프롬프트에 걸쳐 다양한 모델 간의 헤드 투 헤드 성능을 평가한다. 우리는 쌍으로 비교하는 두 주문에서 프롬프트를 시도하고 GPT-4 평가가 일치하지 않으면 결과를 동점으로 계산한다. 또한 805개 이상의 프롬프트로 평가된 AlpacaEval 2.0 리더보드 형식의 결과를 보고하고, GPT-4 판단을 기반으로 기준 GPT-4 터보 모델에 대한 승률을 계산한다.

보상 모델링 섹션 3.1.1에 설명된 대로 Open Assistant 데이터 집합에서 파생된 평가 집합에서 인간 순위와의 상관 관계를 평가한다. 각 명령은 주어진 순위에 대해 평균 2.85개의 응답을 갖는다. 따라서 우리는 모델의 평가와 인간 순위 사이에 주어진 쌍 사이의 순위 순위의 몇 배인 _쌍별 정확도_를 측정할 수 있다. 또한 총 순서가 지침에 대해 정확히 동일한 빈도인 _정확한 일치_ 카운트를 측정합니다. 또한 Spearman correlation과 Kendall의 \(\tau\)을 보고한다. 마지막으로 모델이 5점 만점에 5점이라는 응답이 인간에 의해 가장 높은 순위로 평가되는 빈도를 보고한다.

#### 3.1.3 Training Details

훈련에 따른 지시 우리가 사용하는 훈련 하이퍼파라미터는 다음과 같다. SFT의 경우 학습률 \(5.5e{-6}\)을 사용하여 \(1.1e{-6}\), 배치 크기 16 및 드롭아웃 0.1로 선형적으로 감소한다. 전체 시퀀스 대신 대상 토큰에 대한 손실만 계산한다. DFO의 경우 학습률 \(1e{-6}\), 배치 크기 16, 드롭아웃 0.1, \(\beta\) 값 0.1로 선형적으로 감소하는 학습률 \(1e{-7}\), 배치 크기 16, 드롭아웃 0.1, \(\beta\)을 사용한다. Li 등(2023)에 따라 도출된 253개의 검증 예제에 대해 200단계마다 체크포인트를 저장하고 Claude 2(Anthropic, 2023)를 사용하여 세대를 평가하여 조기 정지를 수행한다. 이것은 AlpacaEval 평가 프롬프트 포맷[Li 등, 2023b]을 사용하여 이전 단계의 세대들에 대해 쌍으로 평가된다.

자체 지침 만들기 새 프롬프트를 생성 하기 위해 8 샷 프롬프트가 있는 고정 모델 Llama 2-Chat 70B를 사용 하는 반면 생성 파이프라인의 다른 부분 (응답 생성 및 평가)은 학습 되는 모델을 사용 합니다. 후보 응답 생성을 위해 온도 \(T=0.7\), \(p=0.9\)의 후보 응답을 샘플링한다. 후보 응답을 평가할 때 이러한 점수에 분산이 있기 때문에 실험에서 샘플링된 디코딩(동일한 매개변수를 사용)을 사용하고 이러한 평가를 여러 번(3) 생성하고 평균을 취한다. 3,964개의 선호도 쌍을 추가하여 DPO를 통해 \(M_{2}\)을 훈련하는 데 사용되는 AIFT(\(M_{1}\)) 데이터 세트를 형성하고, \(M_{3}\)을 훈련하는 데 사용되는 AIFT(\(M_{2}\))를 형성하는 데 6,942개의 선호도 쌍을 추가했다.

### Results

#### 3.2.1 명령어 추적 기능

헤드 투 헤드 성능 결과는 그림 3에 나와 있다.

EFT+IFT 시드 트레이닝은 IFT 단독과 유사하게 수행된다. 본 발명에서는 평가 Fine-Tuning(EFT) 태스크를 트레이닝에 추가하는 것이 거의 동일한 헤드 대 헤드(30.5% 승리 대 30.9% 승리)로 인스트럭션 Fine-Tuning(IFT) 데이터 단독을 사용하는 것에 비해 수행 후 인스트럭션에 영향을 미치지 않는다는 것을 발견한다. 이는 자기 보상에 대한 모델의 증가된 능력이 다른 기술에 영향을 미치지 않는다는 것을 의미하기 때문에 긍정적인 결과이다. 따라서 자기 보상 모델의 반복 1(\(M_{1}\))로 IFT+EFT 훈련을 사용하고 추가 반복을 실행할 수 있다.

반복 2(\(M_{2}\))는 반복 1(\(M_{1}\))보다 향상되었고, SFT 기반 반복 2의 자기 보상 훈련(\(M_{2}\))은 반복 1(\(M_{1}\))에 이어 우수한 지시를 제공하며, 이는 머리 대 머리 평가에서는 \(M_{1}\)의 11.7%에 비해 \(M_{2}\)이 55.5% 승리하였다. SFT 기준선(49.2% 승리 vs. 14.5% 승리)에서도 비슷한 이득을 제공한다. 반복 1의 보상 모델에서 제공하는 선호도 데이터 AIFT(\(M_{1}\))를 사용하여 \(M_{1}\)에서 \(M_{2}\)로 성능이 크게 뛰어남을 분명히 알 수 있다.

반복 3(\(M_{3}\))은 반복 2(\(M_{2}\))에 비해 향상된다. 반복 2에 비해 반복 3에서 추가 이득을 볼 수 있으며, \(M_{3}\)의 경우 47.7%의 승리가 있는 반면 \(M_{2}\)의 경우 12.5%의 승리가 있다. 유사하게, \(M_{3}\)에 대한 SFT 기준선에서의 승률은 62.5% 대 9.8%로 증가하며, 즉 \(M_{2}\) 모델보다 더 자주 승리한다. 전반적으로 반복 2의 보상 모델에서 제공되는 선호도 데이터 AIFT(\(M_{2}\))를 사용한 훈련을 통해 \(M_{2}\)에서 \(M_{3}\)으로 큰 이득을 볼 수 있다.

자기 보상 모델은 AlpacaEval 2 리더보드에서 잘 수행되며, 표 1에 주어진 결과를 통해 AlpacaEval 2.0 리더보드 형식에서 모델을 평가한다. 훈련 반복이 GPT4-터보보다 9.94%, 반복 2에서 15.38%, 반복 3에서 20.44%의 향상된 승리율을 산출한다는 동일한 결과를 관찰한다. 우리의 반복 3 모델은 Claude 2, Gemini Pro 및 GPT4 0613을 포함한 이 메트릭에서 기존의 많은 모델보다 성능이 우수하다. 우리는 표의 리더보드에서 선택된 몇 가지 모델을 보여준다. 이러한 경쟁 모델 중 다수가 독점 정렬 데이터(예: [Touvron et al., 2023]의 1M 주석 이상)를 포함하거나 더 강력한 모델에서 증류된 표적을 사용한다는 점에 주목한다. 대조적으로, 우리의 셀프-보상 모델은 오픈 어시스턴트의 작은 시드 데이터 세트로부터 시작하여, 훈련의 추가 반복을 위해 모델 자체로부터 타겟 및 보상을 생성한다.

선호도 최적화는 긍정적인 예제를 사용하여 증강하는 것보다 성능이 우수하며, 감독된 미세 조정(선호도 최적화 없이)에 고품질 자기 지시 생성 예제를 추가하는 대안적인 자기 훈련 절차도 시도했다. 불행히도 우리는 이 접근법이 도움이 된 구성을 찾을 수 없었다. 예를 들어, 5점 만점에 5점을 기록한 11,254개의 그러한 예를 추가하고 훈련에서 혼합 가중치를 최적화하면 여전히 29% 승수 대 30% 승수의 SFT 베이스라인으로 헤드 대 헤드, 즉 개선이 없다.

데이터 분포 분석은 섹션 A.1에 표시된 IFT, EFT 및 AIFT(\(M_{1}\)) 데이터에 대한 t-SNE(Van der Maaten and Hinton, 2008) 시각화를 수행한다. IFT와 AIFT(\(M_{1}\)) 예제 사이의 좋은 중첩을 발견하며, EFT 예는 임베딩 공간의 다른 부분에 있다. \(M_{1}\)의 세대는 평균 길이가 1092이고, \(M_{2}\)의 세대는 1552, \(M_{3}\)의 세대는 2552이므로 모델이 더 긴 응답을 생성하도록 학습하고 있음을 관찰하며, 이는 상대적 성능의 요인이 될 수 있다.

#### 3.2.2 보상 모델링 능력

보상 모델링 평가 결과는 표 2에 나와 있다.

첫째, 평가 Fine-Tuning(EFT) 데이터를 학습에 추가함으로써 LLM-as-a-Judge의 역할을 하는 방법에 대한 예시를 제공하는 것이 Instruction Fine-Tuning(IFT) 데이터만을 사용한 학습에 비해 자연스럽게 성능이 향상됨을 알 수 있다. IFT 데이터는 광범위한 일반 교육 작업을 포함하며 SFT 기준선에도 응답을 평가할 수 있는 기능이 부여되지만 EFT 데이터는 이 특정 작업에 대한 더 많은 예를 제공한다. IFT+EFT vs. IFT 단독, 예를 들어 인간과의 쌍별 정확도 일치는 65.1%에서 78.7%로 증가한다.

Reward 모델링 능력은 Self-Training을 통해 향상된다. 우리는 일련의 자기 보상 훈련을 수행하는 것이 향상된 명령어 추종 능력에 더하여 다음 반복에 대한 자기 보상을 제공할 때 모델의 능력을 향상시킨다는 것을 발견한다. 모델 \(M_{2}\)(반복 2)는 \(M_{1}\)(반복 1)의 보상 모델을 사용하여 학습되지만, \(M_{1}\)에 비해 5가지 메트릭 모두에서 향상된 성능을 제공한다. 예를 들어, 쌍별 정확도는 78.7%에서 80.4%로 개선된다. 반복 3(\(M_{3}\))은 이러한 메트릭을 몇 가지 더 비교한다.

그림 3: **Self-Training으로 기능 향상:** GPT-4를 사용 하 여 다양 한 프롬프트에서 머리 대 머리 승률을 사용 하 여 모델을 평가 합니다. SFT 기준선은 Self-Rewarding Iteration 1 (\(M_{1}\))과 동등 합니다. 그러나 반복 2 (\(M_{2}\))는 반복 1 (\(M_{1}\))과 SFT 기준선 모두에 비해 성능이 우수하다. 반복 3(\(M_{3}\))은 반복 2(\(M_{2}\)보다 더 많은 이득을 얻을 수 있으며, \(M_{1}\), \(M_{2}\) 및 SFT Baseline을 큰 폭으로 능가한다.

\(M_{2}\)으로, 예를 들어 쌍별 정확도는 80.4%에서 81.7%로 증가한다. 이 성능 이득은 추가 EFT 데이터가 제공 되지 않았음에도 불구하고 달성 되며 _자체 명령 만들기_ 루프 동안 만든 예제는 LLM-as-a-판사 교육 예제와 비슷 하지 않습니다. 우리는 모델이 일반적인 지침에 따라 더 잘 되고 있기 때문에 그럼에도 불구하고 LLM-as-a-판사 작업에서도 개선된다고 가정한다.

이 실험에서 LLM-as-a-Judge 프롬프트의 중요성 그림 2에 표시된 LLM-as-Judge 프롬프트 형식을 사용했다. 예비 실험에서 가장 효과적인 프롬프트를 결정하기 위해 다양한 다른 프롬프트를 시도했다. 예를 들어, 5-포인트 척도를 또한 제안하는 Li 등(2023)에서 제안된 프롬프트를 시도했지만, 옵션들을 다양한 품질 버킷들에서의 객관식으로서 기술하고 있다, 도 5를 참조한다. 대조적으로, 우리의 프롬프트는 품질의 다양한 양태들을 커버하는 가산식으로서 포인트들을 기술한다. 우리는 서로 큰 차이를 발견한다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{Self-Rewarding Models} \\ Model & SFT Baseline & Iter 1 (\(M_{1}\)) & Iter 2 (\(M_{2}\)) & Iter 3 (\(M_{3}\)) \\ \hline Training data & IFT & IFT+EFT & IFT+EFT & IFT+EFT \\  & & & +AIFT(\(M_{1}\)) & +AIFT(\(M_{2}\)) \\ \hline Pairwise accuracy (\(\uparrow\)) & 65.1\% & 78.7\% & 80.4\% & 81.7\% \\
5-best \% (\(\uparrow\)) & 39.6\% & 41.5\% & 44.3\% & 43.2\% \\ Exact Match \% (\(\uparrow\)) & 10.1\% & 13.1\% & 14.3\% & 14.3\% \\ Spearman corr. (\(\uparrow\)) & 0.253 & 0.279 & 0.331 & 0.349 \\ Kendall \(\tau\) corr. (\(\uparrow\)) & 0.233 & 0.253 & 0.315 & 0.324 \\ \hline \hline \end{tabular}
\end{table}
표 2: **자체 훈련으로 보상 모델링 능력이 향상됨**: 보류된 인간 선호도 데이터와의 정렬을 측정하는 다양한 메트릭을 통해 LLM-as-a-판단을 평가합니다. 자기 보상 반복 2(모델 \(M_{2}\))는 이전 반복에서 유도된 자기 보상 모델을 사용하여 학습되는 Iteration 1(모델 \(M_{1}\)보다 성능이 우수한 반면, \(M_{1}\)은 명령어 미세 조정(IFT) 데이터에서만 학습된 표준 SFT 기준 모델보다 성능이 우수하다. 반복 3(모델 \(M_{3}\))은 반복 2에 비해 추가 개선을 제공한다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{Alignment Targets} \\ Model & **Win Rate** & Distilled & Proprietary \\ \hline Self-Rewarding 70B & & & \\ _Iteration 1_ (\(M_{1}\)) & 9.94\% & & \\ _Iteration 2_ (\(M_{2}\)) & 15.38\% & & \\ _Iteration 3_ (\(M_{3}\)) & 20.44\% & & \\ \hline \hline \end{tabular}
\begin{tabular}{l l l l} \hline \hline \multicolumn{4}{l}{_Selected models from the leaderboard_} \\ GPT-4 0314 & 22.07\% & & ✓ \\ Mistral Medium & 21.86\% & & ✓ \\ Claude 2 & 17.19\% & & ✓ \\ Gemini Pro & 16.85\% & & ✓ \\ GPT-4 0613 & 15.76\% & & ✓ \\ GPT 3.5 Turbo 0613 & 14.13\% & & ✓ \\ LLaMA2 Chat 70B & 13.87\% & & ✓ \\ Vicuna 33B v1.3 & 12.71\% & ✓ & \\ Humpback LLaMa2 70B & 10.12\% & & \\ Guanaco 65B & 6.86\% & & \\ Davinci001 & 2.76\% & & ✓ \\ Alpaca 7B & 2.59\% & ✓ & \\ \hline \hline \end{tabular}
\end{table}
표 1: **AlpacaEval 2.0 결과** (GPT-4에 의해 평가된 GPT-4 터보보다 승률) 자체 보상을 반복하면 승률이 향상됩니다. 반복 3(\(M_{3}\))은 독점적인 학습 데이터를 사용하거나 더 강한 모델에서 증류된 타겟을 사용하는 기존의 많은 모델보다 성능이 우수하다.

이 두 가지 프롬프트는 SFT 기준선을 사용할 때, 예를 들어 우리의 경우 65.1%의 쌍별 정확도와 그들의 경우 26.6%의 쌍별 정확도이다. 자세한 내용은 섹션 A.2를 참조하십시오.

## 4 관련 작업

자동으로 대형 언어 모델을 개선하거나 스스로 수정하는 것이 연구의 주요 초점이 되고 있다. Pan 등(2023)의 최근 조사는 주제를 요약하려고 시도한다. 그러나 이곳은 빠르게 움직이는 지역이며 이미 그곳에서 다루지 않은 유망한 새로운 작품이 있습니다.

인간 피드백(RLHF)으로부터의 강화 학습 Ziegler 등(2019); Stiennon 등(2020); Ouyang 등(2022); Bai 등(2022)은 인간의 선호 데이터로부터 고정된 보상 모델을 트레이닝한 다음, 보상 모델을 사용하여 강화 학습(RL)을 통해, 예를 들어 Proximal Policy Optimization(PPO)을 통해 트레이닝한다(Schulman 등, 2017). 따라서 어떤 의미에서 보상 신호는 이미 이러한 작업에서도 모델에서 비롯되지만 인간의 데이터에서 증류된다. 그럼에도 불구하고, 이는 일반적으로 인간 피드백(RLHF)으로부터의 RL로 지칭된다. 직접 선호도 최적화(Direct Preference Optimization, DPO)(Rafailov et al., 2023)와 같은 방법들은 보상 모델을 완전히 트레이닝하는 것을 피하고, 대신에 인간의 선호도를 이용하여 LLM을 직접 트레이닝한다. Pairwise Cringe Optimization (PCO) (Xu et al., 2023)을 포함하여, 몇몇 다른 그러한 경쟁 방법들이 또한 존재한다 (Zhao et al., 2023; Zheng et al., 2023; Yuan et al., 2023). 이는 AlpacaFarm 상에서 DPO 및 PPO를 능가하는 것으로 나타났다 (Dubois et al., 2023). PCO는 고정된 보상 모델을 제외하고 우리 작업의 것과 유사한 반복 훈련 접근법을 사용하고 또한 반복 DPO가 동일한 스킴을 사용하여 DPO보다 개선됨을 보여주었다.

Reinforcement Learning from AI Feedback(RLAIF)Constitutional AI(Bai et al., 2022)는 LLM을 이용하여 피드백을 주고 반응을 정제하고, 이 데이터를 이용하여 (분리, 고정) 보상 모델을 학습시킨다. 이어서, 이는 "AI 피드백으로부터의 RLAIF"(RLAIF)를 수행하기 위해 사용된다. Lee et al.(2023)은 RLAIF와 RLHF 절차를 비교하고 그들이 비교하는 방법이 대략 동일하게 수행됨을 찾는다. 그들은 "선반이 없는" LLM을 사용하여 LLM-as-a-Judge 프롬프팅을 수행하여 고정된 보상 모델을 훈련하기 위한 훈련 세트를 구축하고, 이를 RL 훈련에 사용한다. 그들은 또한 고정적이지만 별도의 LLM-as-a-Judge 모델을 직접 사용하는 실험을 하는데, 저자들이 보고하는 것은 PPO 훈련 내에서 사용하기 때문에 계산 비용이 많이 든다(우리가 작업에서 사용하는 반복적인 접근법의 오프라인 단계보다는 상대적으로 계산 비용이 많이 든다). 마지막으로 Chen 등(2024)은 최근 한 쌍의 승리 응답으로 인간 레이블을 사용하고 한 쌍의 패배 응답으로 마지막 반복의 세대를 사용하여 Iterative DPO와 같은 프레임워크에서 보상 모델을 완전히 피할 수 있음을 보여주었다. 저자들은 이것이 모델 세대가 인간 성능에 도달하면 병목 현상이 발생한다는 한계가 있다는 점에 주목한다. 또한 각 입력 프롬프트에는 작업과 달리 사람이 주석을 달아야 한다.

데이터 증강(및 큐레이션)을 통해 LLM을 개선하는 여러 방법은 미세 조정을 강화하기 위해 훈련 데이터를 (자체) 생성함으로써 LLM을 개선했다. 셀프-인스트럭션(Wang et al., 2022)은 프롬프트 및 응답의 셀프-인스트럭션 생성을 위한 방법으로서, 베이스 LLM을 개선하는데 사용될 수 있다. 우리는 우리의 작업에서 유사한 기술을 사용하고 나서 자기 보상 모델을 사용하여 점수를 매긴다. 또한 몇 가지 접근 방식은 강력한 LLM에서 증류하여 훈련 데이터를 생성했으며 약한 LLM이 잘 수행할 수 있음을 보여주었다. 예를 들어, 알파카(Taori et al., 2023)는 셀프 인스트럭션의 스타일로 생성된 텍스트-다빈치-003 인스트럭션으로 라마 7B 모델을 미세 조정하였다. Alpagasus(Chen et al., 2023)는 강력한 LLM-as-a-Judge(ChatGPT)를 사용하여 Alpaca 데이터세트를 큐레이트하고 더 작은 세트로 필터링하여 개선된 결과를 얻었다. 명령어 역번역(Li 등, 2023)은 유사하게 트레이닝 데이터를 증강하고 큐레이트하지만, 프롬프트를 예측하기 위해 웹 문서로부터의 역번역을 통해 증강한다. 큐레이션은 LLM(-as-a-Judge) 자체에 의해 이루어지기 때문에 자기 보상 모델의 한 예이지만 특화된 환경에서 이루어진다고 볼 수 있다. Reinforced Self-Training(ReST)(Gulcehre et al., 2023)은 고정된 외부 보상을 사용하여 새로운 고품질 예들을 큐레이트하여 훈련 세트에 반복적으로 추가하여 성능을 향상시킨다. 우리는 실험에서 관련 방식으로 긍정적인 예만 추가하는 것이 도움이 되지 않는 반면 선호도 쌍을 추가하는 것이 도움이 된다는 것을 발견했다.

LLM-as-a-JudgeUsing LLM-as-a-Judge prompting to evaluate language models is become a standard approach (Dubois et al., 2023; Li et al., 2023; Fernandes et al., 2023; Bai et al., 2023; Saha et al., 2023) 와 같이, 보상 모델들을 훈련시키거나 데이터를 큐레이트하는데도 사용되고 있다 (Lee et al., 2023; Chen et al., 2023; Li et al., 2023a). Kim 등(2023)과 같은 일부 작업은 LLM이 판사로서 잘 수행되도록 훈련하기 위해 데이터에 대한 훈련을 생성하지만, 우리가 아는 한 이 훈련을 우리의 작업에서와 같이 일반적인 지시 추종 기술과 결합하는 것은 일반적이지 않다.

## 5 Conclusion

본 논문에서는 자기 보상 언어 모델(Self-Rewarding Language Model)을 소개한다. 자기 보상 언어 모델은 자기 세대의 판단과 훈련을 통해 자기 정렬이 가능한 모델이다. 방법은 반복 방식으로 트레이닝되며, 여기서 각각의 반복에서 모델은 자체 선호도 기반 명령어 트레이닝 데이터를 생성한다. 이는 LLM-as-a-판정 프롬프팅을 통해 자체 세대에 보상을 할당하고 반복 DPO를 사용하여 선호도를 교육함으로써 수행됩니다. 우리는 이 훈련이 모델의 명령어 추종 능력과 반복에 걸친 보상 모델링 능력을 모두 향상시킨다는 것을 보여주었다. 이것은 단지 예비 연구일 뿐이지만, 우리는 이것이 흥미로운 연구의 길이라고 믿는다. 왜냐하면 이것은 모델이 일종의 선순환을 따르는 수업을 개선하기 위해 향후 반복에서 보상을 더 잘 할당할 수 있다는 것을 의미하기 때문이다. 이 개선은 현실적인 시나리오에서 포화될 가능성이 있지만, 오늘날 모델에 따른 보상 모델 및 지침을 구축하는 데 일반적으로 사용되는 인간의 선호도를 넘어서는 지속적인 개선의 가능성을 여전히 허용한다.

## 6 Limitations

유망한 실험 결과를 얻었지만, 안전성 평가를 포함한 추가 평가 주제 및 반복 훈련의 한계를 이해하기 위해 아직 탐색하지 않은 방법이 많기 때문에 현재 예비적인 것으로 간주한다.

훈련의 반복은 수업 추종 능력과 보상 모델링 능력을 모두 향상시켰지만 단일 설정에서 3번의 반복만 실행되었음을 보여주었다. 추가 연구의 명확한 선은 더 많은 반복과 다른 설정에서 다소 기능이 있는 다른 언어 모델 모두에 대해 이 효과의 "스케일링 법칙"을 이해하는 것이다.

GPT-4를 사용하여 머리 대 머리 및 알파카에발 2 리더보드 스타일 평가를 사용하여 모델을 평가했지만 측정할 수 있는 다른 많은 자동 평가 벤치마크가 있다. 또한 모델 세대에서 길이의 증가를 관찰했으며 길이와 추정된 품질 사이에는 알려져 있는 상관관계가 있으며, 이는 일반적으로 더 깊이 이해해야 하는 주제이며 특히 우리의 결과에서도 마찬가지이다. 우리의 틀 안에서 소위 "보상-해킹"이 일어날 수 있는지, 그리고 어떤 상황에서 일어날 수 있는지도 이해하는 것이 좋을 것이다. 학습 보상으로 언어 모델과 최종 평가를 위한 언어 모델을 모두 사용하고 있기 때문에 서로 다른 모델일지라도 이는 우리가 제공한 것보다 더 깊은 분석이 필요할 수 있다. 우리는 우리가 보는 자동 결과를 검증하는 예비 인간(저자) 평가를 수행했지만 더 자세한 인간 평가가 도움이 될 것이다.

또 다른 명확한 추가 연구 방법은 안전 평가를 수행하고 프레임워크 내에서 안전 교육을 탐색하는 것이다. 보상 모델은 기존 시스템에서 안전을 위해 독점적으로 구축되었으며(Touvron et al., 2023), 여기에서 유망한 방법은 LLM-as-a-Judge 절차를 사용하여 자체 보상 훈련 프로세스에서 특히 안전을 평가하는 것이다. 보상 모델링 능력이 훈련 반복보다 향상된다는 것을 보여주었다는 점을 감안할 때, 이는 모델의 안전성이 시간이 지남에 따라 잠재적으로 향상될 수 있음을 의미할 수 있으며, 이후 반복은 이전 반복이 수행할 수 없는 더 어려운 안전 상황을 포착할 수 있다.

## References

* Achiam 등(2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, 등 GPT-4 기술 보고서. arXiv preprint arXiv:2303.08774, 2023.
* Adolphs 등(2023) Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. CRINGE 손실: 모델링하지 않을 언어를 학습합니다. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editor, _Proceedings of the 61th Annual Meeting of the Association of Computational Linguistics(Volume 1: Long Papers)_, pages 8854-8874, pages, Canada, Toronto, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.493. URL [https://aclanthology.org/2023.acl-long.493](https://aclanthology.org/2023.acl-long.493)
* 인류(2023) 인류. Claude 2. [https://www.anthropic.com/index/claude-2](https://www.anthropic.com/index/claude-2), 2023.
* Bai et al. (2022a) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.
* Bai 등(2022b) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, 등. Constitutional AI: AI 피드백으로부터의 유해성. arXiv preprint arXiv:2212.08073, 2022b.
* Bai 등(2023) Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 언어 모델 검사자와 함께 기초 모델을 벤치마킹합니다. 17차 신경 정보 처리 시스템 데이터 세트 및 Benchmarks Track 회의에서 2023. URL [https://openreview.net/forum?id=IiRHQ7gvnq](https://openreview.net/forum?id=IiRHQ7gvnq).
* Chen 등(2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: 더 적은 데이터로 더 나은 알파카를 훈련시킨다. arXiv preprint arXiv:2307.08701, 2023.
* Chen 등(2024) Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 셀프 플레이 미세 조정은 약한 언어 모델을 강한 언어 모델로 변환합니다. arXiv preprint arXiv:2401.01335, 2024.
* Collobert and Weston (2008) Ronan Collobert and Jason Weston. 자연어 처리를 위한 통합 아키텍처: 멀티태스크 학습이 포함된 심층 신경망입니다. Proceedings of the 25th International Conference on Machine Learning, pp. 160-167, 2008.
* Dubois 등(2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 알파카팜: 인간의 피드백으로부터 학습하는 방법들에 대한 시뮬레이션 프레임워크. arXiv preprint arXiv:2305.14387, 2023.
* 페르난데스 등(2023) Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andre FT Martins, Graham Neubig, Ankush Garg, Jonathan H Clark, Markus Freitag, and Orhan Firat. 악마는 오류에 있다: 세밀한 기계 번역 평가를 위해 큰 언어 모델을 활용한다. arXiv preprint arXiv:2308.07286, 2023.
* Gulcehre et al.(2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training(rest). arXiv preprint arXiv:2308.08998, 2023.
* Honovich 등(2023) 또는 Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 부자연스러운 지침: 인간 노동력이 거의 없는 언어 모델을 조정합니다. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61th Annual Meeting of the Association of Computational Linguistics(Volume 1: Long Papers)_, pages 14409-14428, pages of the Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806. URL [https://aclanthology.org/2023.acl-long.806](https://aclanthology.org/2023.acl-long.806)
* Held et al. (2013) Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Sungjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. _ arXiv preprint arXiv:2310.08491_, 2023.
* Kopf et al. (2023) Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. OpenAssistant conversations-democratizing large language model alignment. _ arXiv preprint arXiv:2304.07327_, 2023.
* Lee 등(2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: ai 피드백을 갖는 인간 피드백으로부터의 스케일링 강화 학습. _ arXiv preprint arXiv:2309.00267_, 2023.
* Li 등(2023a) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 명령어 역번역과의 자체 정렬 _ arXiv preprint arXiv:2308.06259_, 2023a.
* Li 등(2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: 명령어 추종 모델의 자동 평가기. [https://github.com/tatsu-lab/alpaca_eval] (https://github.com/tatsu-lab/alpaca_eval), 2023b.
* Ouyang et al.(2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _ Advance in Neural Information Processing Systems_, 35:27730-27744, 2022.
*Pan 등(2023) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 대용량 언어 모델 자동 수정: 다양한 자기 수정 전략의 풍경 조사. _ arXiv preprint arXiv:2308.03188_, 2023.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are nonsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.
* Rafailov 등(2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. _30-7번째 신경 정보 처리 시스템 회의_에서 2023. URL [https://openreview.net/forum?id=HPuSIXJaa9](https://openreview.net/forum?id=HPuSIXJaa9)입니다.
* Saha 등(2023) Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 분기 해결 병합은 대규모 언어 모델 평가 및 생성을 향상시킵니다. _ arXiv preprint arXiv:2310.15123_, 2023.
* Schulman 등(2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 근위 정책 최적화 알고리즘입니다. _ arXiv preprint arXiv:1707.06347_, 2017.
* Stiennon 등(2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 인간의 피드백으로 요약하는 방법을 배웁니다. _ Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* Taori 등(2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: 명령어 추종 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.
* Touvron 등(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. t-SNE를 사용하여 데이터를 시각화합니다. _ Journal of machine learning research_, 9(11), 2008.
* Wang 등(2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannanehajishirzi. 자체 지시: 언어 모델을 자체 생성 지침에 정렬합니다. _ arXiv preprint arXiv:2212.10560_, 2022.
* Wang 등(2021)* Xu 등(2023) Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, Jason Weston. 어떤 것은 다른 것 보다 더 움츠러듭니다. 쌍별 범죄 손실을 사용 하는 선호도 최적화 _ arXiv preprint arXiv:2312.16682_, 2023.
* Yuan 등(2023) Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: 언어 모델을 인간 피드백과 정렬하기 위한 응답 순위. _30-7번째 신경 정보 처리 시스템 회의_에서 2023. URL [https://openreview.net/forum?id=EdIGMCHk4l](https://openreview.net/forum?id=EdIGMCHk4l).
* Zhao 등(2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: 인간 피드백을 갖는 시퀀스 가능성 교정. _ arXiv preprint arXiv:2305.10425_, 2023.
* Zheng et al.(2023) Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. 클릭: 시퀀스 가능성 대비 학습으로 제어 가능한 텍스트 생성입니다. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editor, _Findings of the Association for Computational Linguistics: ACL 2023_, pages 1022-1040, Toronto, Canada, July 2023a. 계산 언어학을 위한 연관성. doi: 10.18653/v1/2023.findings-acl.65. URL [https://aclanthology.org/2023.findings-acl.65](https://aclanthology.org/2023.findings-acl.65)
* Zheng 등(2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhang, Zhanghao Wu, Yonghao Zhang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. MT 벤치 및 챗봇 경기장으로 LLM-as-a-judge를 판단합니다. _37번째 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙에 관한 회의_, 2023b. URL [https://openreview.net/forum?id=uccHPGDlao](https://openreview.net/forum?id=uccHPGDlao)입니다.
* Ziegler 등(2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 인간 선호도로부터 언어 모델을 미세 조정합니다. _ arXiv preprint arXiv:1909.08593_, 2019.

Appendix

### IFT, EFT 및 AIFT 데이터 배포

IFT, EFT 및 AIFT 데이터에 대한 지침의 분포와 IFT, EFT 및 AIFT 데이터에 대한 응답 분포를 그림 4에 표시했다. IFT 데이터와 EFT 데이터는 매우 다른 분포에서 나온 반면 IFT 및 AIFT 데이터는 유사한 분포에서 나온다는 것은 분명하다.

### 시도된 다른 EFT 프롬프트

먼저 그림 5와 같이 Li 등(2023a)의 EFT 프롬프트를 사용하였다. 그러나 이 프롬프트는 모델이 과제를 객관식 문제로 다루는 데 필요했기 때문에 가산 점수 계산 프롬프트만큼 효과적이지 않았으며 모델이 이 객관식 문제를 응답의 다양한 측면을 평가하는 것과 관련된 하위 문제로 분해하는 것이 어렵다는 것을 발견했다. 3,200개의 IFT 데이터에 대해서만 학습된 모델을 사용할 때 Li 등(2023a)의 추가 점수 계산 프롬프트 및 프롬프트를 사용하여 EFT 테스트 세트에 대한 성능은 표 3에 나와 있다.

\begin{table}
\begin{tabular}{l c c} \hline \hline EFT Prompt & Multiple Choice prompt & Ours \\ \hline Pairwise accuracy (\(\uparrow\)) & 26.6\% & 65.1\% \\
5-best \% (\(\uparrow\)) & 23.5\% & 39.6\% \\ Exact Match \% (\(\uparrow\)) & 1.1\% & 10.1\% \\ Spearman corr. (\(\uparrow\)) & -0.18 & 0.25 \\ Kendall \(\tau\) corr. (\(\uparrow\)) & -0.16 & 0.23 \\ \hline \hline \end{tabular}
\end{table}
표 3: 3,200개의 IFT 데이터만으로 학습된 모델을 사용하여 다양한 LLM-as-Judge 프롬프트를 시도했으며 Li 등(2023a)이 사용한 프롬프트에 비해 EFT 성능이 크게 향상되었음을 보여주는 추가 점수 계산 프롬프트가 가장 잘 작동한다는 것을 발견했다.

그림 4: IFT, EFT 및 AIFT 데이터에 대한 지침과 응답의 배포.

아래는 사용자로부터의 질문 및 후보 응답이다. 다음 기준을 사용하여 응답을 5점 척도로 채점하십시오.

1: 답변이 불완전하거나, 모호하거나, 주제에서 벗어나거나, 논란이 있거나, 사용자가 요청한 것이 정확히 아니라는 것을 의미한다. 예를 들어, 일부 콘텐츠가 누락된 것처럼 보이고, 번호가 매겨진 리스트는 처음부터 시작되지 않고, 시작 문장은 사용자의 질문을 반복한다. 또는 응답은 (예를 들어, 블로그 게시물에서 가져온) 개인적인 경험을 가진 다른 사람의 관점에서 또는 포럼의 답변처럼 보인다. 또는 홍보 텍스트, 탐색 텍스트 또는 기타 관련 없는 정보가 포함되어 있습니다.

2: 사용자로부터의 대부분의 질문에 대한 답변을 의미한다. 사용자의 질문을 직접 다루지 않습니다. 예를 들어, 사용자의 질문에 대한 정확한 솔루션 대신 높은 수준의 방법론만 제공합니다.

3: 답변이 도움이 되지만 AI 어시스턴트에 의해 작성되지 않는다는 것을 의미한다. 사용자로부터의 모든 기본 요청을 처리합니다. 대응이 AI 비서의 관점에서 작성되는 것이 아니라 다른 사람의 관점에서 작성된다는 단점과 함께 완전하고 자아적이다. 내용은 블로그 게시물, 웹 페이지 또는 웹 검색 결과에서 발췌한 것처럼 보입니다. 예를 들어 개인적인 경험이나 의견, 댓글 부분을 언급하거나 소셜 미디어에 공유하는 내용 등을 담고 있다.

4: 지시를 다루는 것에 명확한 초점을 두고 AI 어시스턴트의 관점에서 답변을 작성한다는 것을 의미한다. 사용자의 질문이나 지시에 대해 누락되거나 관련 없는 정보 없이 완전하고 명확하며 포괄적인 응답을 제공합니다. 잘 정리되어 있고, 자립적이며, 도움이 되는 어조로 쓰여져 있습니다. 예를 들어, 더 간결하고 집중되는 등 개선의 여지가 미미합니다.

5: AI 어시스턴트의 완벽한 답변이라는 것을 의미한다. 관련 없는 문장 없이 사용자의 질문이나 지시를 해결하기 위해 응답이 의도적으로 작성된 것처럼 보이는 유용한 AI 어시스턴트가 되는 데 분명한 초점을 맞추고 있다. 답은 해당 분야의 전문 지식을 보여주는 고품질 콘텐츠를 제공하며 매우 잘 작성되고 논리적이며 따라하기 쉽고 매력적이며 통찰력이 있다.

User: <INSTRUCTION_HERE>

<response><RESPONSE_HERE></response>

먼저 당신의 추론(100단어 이하)을 간략하게 설명한 다음 마지막 줄에 "점수:<평가>"를 적어주세요. 필요한 경우 웹 검색의 지식을 사용하여 AI 어시스턴트 스타일로 답변하십시오. 기준에 따라 최종 점수를 도출하기 위해 단계별로 생각해 보자.

도 5: Li 등으로부터 취해진 LLM-as-a-판정 프롬프트. [2023a].
